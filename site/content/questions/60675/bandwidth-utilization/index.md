+++
type = "question"
title = "Bandwidth utilization"
description = '''Here is my setup - I have one sender, one receiver and one monitor (in between the sender and the receiver - acting as a router ) - the sender and receiver communicate through the monitor.  However, when I measure the throughput at both ends, i am not able to utilize the whole maximum bandwidth (cap...'''
date = "2017-04-09T12:45:00Z"
lastmod = "2017-04-10T02:07:00Z"
weight = 60675
keywords = [ "segmentation", "bandwidth", "tcp", "bandwidthutilization" ]
aliases = [ "/questions/60675" ]
osqa_answers = 2
osqa_accepted = false
+++

<div class="headNormal">

# [Bandwidth utilization](/questions/60675/bandwidth-utilization)

</div>

<div id="main-body">

<div id="askform">

<table id="question-table" style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><div id="post-60675-score" class="post-score" title="current number of votes">0</div><div id="favorite-count" class="favorite-count"></div></div></td><td><div id="item-right"><div class="question-body"><p>Here is my setup - I have one sender, one receiver and one monitor (in between the sender and the receiver - acting as a router ) - the sender and receiver communicate through the monitor.</p><p>However, when I measure the throughput at both ends, i am not able to utilize the whole maximum bandwidth (capacity) available. For example, withe the given bandwidth,delay and loss - we normally expect our throughput to be around 250mbit/s or above. However, I am getting about 124mbits/s at both ends and i don't think this is realistic both theoretically and practically.</p><p>How can we maximize the maximum capacity when we turn the segmentation OFF? I would be so much grateful if you could give me some hints on this.</p></div><div id="question-tags" class="tags-container tags">segmentation bandwidth tcp bandwidthutilization</div><div id="question-controls" class="post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>asked <strong>09 Apr '17, 12:45</strong></p><img src="https://secure.gravatar.com/avatar/6dd3e71b974fad46455a71063cb9c319?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="armodes&#39;s gravatar image" /><p>armodes<br />
<span class="score" title="16 reputation points">16</span><span title="18 badges"><span class="badge1">●</span><span class="badgecount">18</span></span><span title="19 badges"><span class="silver">●</span><span class="badgecount">19</span></span><span title="23 badges"><span class="bronze">●</span><span class="badgecount">23</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="armodes has no accepted answers">0%</span></p></div><div class="post-update-info post-update-info-edited"><p>edited 10 Apr '17, 18:00</p></div></div><div id="comments-container-60675" class="comments-container"><span id="60676"></span><div id="comment-60676" class="comment"><div id="post-60676-score" class="comment-score"></div><div class="comment-text"><p>Maybe it's the segmentation that is required to achieve maximum speed - I mean, it's a performance feature, so it's not that surprising that you don't reach max speeds without it...</p></div><div id="comment-60676-info" class="comment-info"><span class="comment-age">(09 Apr '17, 12:57)</span> Jasper ♦♦</div></div><span id="60677"></span><div id="comment-60677" class="comment"><div id="post-60677-score" class="comment-score"></div><div class="comment-text"><p>Where do capture, inside or outside the offloading machines? From my point of view an internal capture is by far not trustworthy.</p></div><div id="comment-60677-info" class="comment-info"><span class="comment-age">(09 Apr '17, 13:00)</span> Christian_R</div></div><span id="60678"></span><div id="comment-60678" class="comment"><div id="post-60678-score" class="comment-score"></div><div class="comment-text"><p>I agree with @Christian_R - when you're investigating issues like this you should go at least for a SPAN port capture. Better would be a full duplex capture using a TAP and a high precision capture card, of course, but that's expensive.</p></div><div id="comment-60678-info" class="comment-info"><span class="comment-age">(09 Apr '17, 13:03)</span> Jasper ♦♦</div></div><span id="60679"></span><div id="comment-60679" class="comment"><div id="post-60679-score" class="comment-score"></div><div class="comment-text"><p>@christian_r, i am turning the offloading OFF on both the sender, receiver and the monitor.</p></div><div id="comment-60679-info" class="comment-info"><span class="comment-age">(09 Apr '17, 13:04)</span> armodes</div></div><span id="60680"></span><div id="comment-60680" class="comment"><div id="post-60680-score" class="comment-score"></div><div class="comment-text"><p>You can do this. But then you have manipulated the whole network setup. So best thing is to trace external. If you can't do then disabling offloading maybe a workaround. Can share us the trace? BTW in your example it looks like you have disabled offloading. How did you generate the traffic?</p></div><div id="comment-60680-info" class="comment-info"><span class="comment-age">(09 Apr '17, 13:38)</span> Christian_R</div></div><span id="60681"></span><div id="comment-60681" class="comment not_top_scorer"><div id="post-60681-score" class="comment-score"></div><div class="comment-text"><p>@christian_r, yes I have disabled offloading and I am generating the traffic using iperf and capturing the traffic on the monitor (between the sender and the receiver)</p></div><div id="comment-60681-info" class="comment-info"><span class="comment-age">(09 Apr '17, 13:58)</span> armodes</div></div><span id="60682"></span><div id="comment-60682" class="comment not_top_scorer"><div id="post-60682-score" class="comment-score"></div><div class="comment-text"><p>O.k. Do you generate UDP or TCP traffic?</p></div><div id="comment-60682-info" class="comment-info"><span class="comment-age">(09 Apr '17, 14:11)</span> Christian_R</div></div><span id="60684"></span><div id="comment-60684" class="comment not_top_scorer"><div id="post-60684-score" class="comment-score"></div><div class="comment-text"><p>@christian_r, i have tried it with both TCP and UDP. Here is my trace file:</p><p><a href="https://drive.google.com/drive/folders/0B4Ajk8jGD1OxbS15amlQZGVFTk0?usp=sharing">https://drive.google.com/drive/folders/0B4Ajk8jGD1OxbS15amlQZGVFTk0?usp=sharing</a></p></div><div id="comment-60684-info" class="comment-info"><span class="comment-age">(09 Apr '17, 14:15)</span> armodes</div></div><span id="60685"></span><div id="comment-60685" class="comment not_top_scorer"><div id="post-60685-score" class="comment-score"></div><div class="comment-text"><p>OK: And in TCP and UDP you got the results?</p></div><div id="comment-60685-info" class="comment-info"><span class="comment-age">(09 Apr '17, 14:43)</span> Christian_R</div></div><span id="60687"></span><div id="comment-60687" class="comment not_top_scorer"><div id="post-60687-score" class="comment-score"></div><div class="comment-text"><p>@christian_r, yes but the problem is the maximum bandwidth utilization is the same with both TCP and UDP.</p></div><div id="comment-60687-info" class="comment-info"><span class="comment-age">(09 Apr '17, 14:46)</span> armodes</div></div></div><div id="comment-tools-60675" class="comment-tools"><span class="comments-showing"> showing 5 of 10 </span> <a href="#" class="show-all-comments-link">show 5 more comments</a></div><div class="clear"></div><div id="comment-60675-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

------------------------------------------------------------------------

<div class="tabBar">

<span id="sort-top"></span>

<div class="headQuestions">

2 Answers:

</div>

</div>

<span id="60692"></span>

<div id="answer-container-60692" class="answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><div id="post-60692-score" class="post-score" title="current number of votes">1</div></div></td><td><div class="item-right"><div class="answer-body"><p>I might be wrong but I'm guessing both sender and receiver are running Linux with CONFIG_HZ=250 in a Virtual Box environment.<br />
The MTU size along the path is size is 1500.<br />
The receiver is slow in delivering the acknowledgements compared to the 1 ms that you defined in netem<br />
</p><p>My suggestions to get a better throughput would be to<br />
1. Define a MTU size of 9000 on all interfaces, sender, bride, receiver<br />
2. If you can/dare, change the kernel clock rate from 250Hz to 1000Hz to get a better than 4ms granularity</p><p>Regards Matthias</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>09 Apr '17, 22:50</strong></p><img src="https://secure.gravatar.com/avatar/5500bd1decb766660522dfb347eedc49?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="mrEEde&#39;s gravatar image" /><p>mrEEde<br />
<span class="score" title="3892 reputation points"><span>3.9k</span></span><span title="15 badges"><span class="badge1">●</span><span class="badgecount">15</span></span><span title="22 badges"><span class="silver">●</span><span class="badgecount">22</span></span><span title="70 badges"><span class="bronze">●</span><span class="badgecount">70</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="mrEEde has 48 accepted answers">20%</span> </br></br></p></div></div><div id="comments-container-60692" class="comments-container"><span id="60695"></span><div id="comment-60695" class="comment"><div id="post-60695-score" class="comment-score"></div><div class="comment-text"><p>OK i will do that Matthias but the reason why I have MTU size as 1500 is because i turned the TCP segmentation offload OFF.</p></div><div id="comment-60695-info" class="comment-info"><span class="comment-age">(10 Apr '17, 00:45)</span> armodes</div></div><span id="60696"></span><div id="comment-60696" class="comment"><div id="post-60696-score" class="comment-score"></div><div class="comment-text"><p>If this is all a virtual environment and you are not going out via real ethernet cards , there is not much benefit in saving the CPU cycles at the sender just to have them burned in the host's operating syxstem. It won't make things faster.</p></div><div id="comment-60696-info" class="comment-info"><span class="comment-age">(10 Apr '17, 00:55)</span> mrEEde</div></div><span id="60706"></span><div id="comment-60706" class="comment"><div id="post-60706-score" class="comment-score"></div><div class="comment-text"><p>The main purpose of offloading segmentation is to save CPU cycles at the sender by having the ethernet card do this.</p><p>From a performance perspective it is always best to send data in as large segments as possible. So with 1500 MTU you need to send roughly 6 times more packets than with 9000 which puts unnecessary load on the host and on the receiver. . if you are CPU constrained already in this situation - which is what I think is limiting performanec here - both enabling segmentation offload and using MTU 1500 is detrimental to gaining good performance.</p></div><div id="comment-60706-info" class="comment-info"><span class="comment-age">(10 Apr '17, 08:16)</span> mrEEde</div></div></div><div id="comment-tools-60692" class="comment-tools"></div><div class="clear"></div><div id="comment-60692-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<span id="60699"></span>

<div id="answer-container-60699" class="answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><div id="post-60699-score" class="post-score" title="current number of votes">1</div></div></td><td><div class="item-right"><div class="answer-body"><p>The throughput limitation here appears to be due to the sending client.</p><p>The server almost constantly advertises a Receive Window of 1.5 MB but the client seems to transmit sometimes in fast bursts (but never filling the 1.5 MB window) but mostly at a slow rate. The Transmit Window never gets above 200-300 KB. This means that Window Scaling is being used - so we can eliminate the 64 KB per round trip limitation.</p><p>Some of this does seem related to transmit window "halving" in response to packet losses, but even without losses, we get a transmitted flow as in the attached TCP Stream diagram.</p><p>There are far more "slow" transmissions than "fast" bursts.</p><p><img src="https://osqa-ask.wireshark.org/upfiles/Offloading-Off-300Mb_2ULcAw5.png" alt="alt text" /></p><p>Notice also that the horizontal distance from the data packets to the ACK line indicate varying RTTs. They range from 3 ms to 13 ms. Actually, looking more deeply, this is due to there being large time gaps between the server's ACKs. There isn't the usual ACK for every two packets. I'd say that the server's ACKs have been dropped by the sniffer (given that there are lots of missing data packets in this trace file that were also dropped by the capture mechanism).</p><p>It would seem to me that there is no TCP level reason for the slow transmit rate. You need to be looking at your client and why it would slow its output in this way. That is, could there be an application layer bottleneck?</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>10 Apr '17, 02:07</strong></p><img src="https://secure.gravatar.com/avatar/35a0c1d0cf15b9d54d73bf54ae28abcd?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="Philst&#39;s gravatar image" /><p>Philst<br />
<span class="score" title="431 reputation points">431</span><span title="1 badges"><span class="badge1">●</span><span class="badgecount">1</span></span><span title="6 badges"><span class="silver">●</span><span class="badgecount">6</span></span><span title="16 badges"><span class="bronze">●</span><span class="badgecount">16</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="Philst has 6 accepted answers">27%</span> </br></br></p></img></div><div class="post-update-info post-update-info-edited"><p>edited 10 Apr '17, 17:04</p></div></div><div id="comments-container-60699" class="comments-container"><span id="60701"></span><div id="comment-60701" class="comment"><div id="post-60701-score" class="comment-score"></div><div class="comment-text"><p>The reason why we get window halving due to the TCP variant i am using (CUBIC). I can't understand why the sending client is behaving that way. Turning the window scaling is also not a good option :( How can i reproduce that graph by the way? Statistics -&gt; TimeSteamGraphs -&gt; TimeSequence (tcptrace) is not giving me the same graph.</p></div><div id="comment-60701-info" class="comment-info"><span class="comment-age">(10 Apr '17, 04:43)</span> armodes</div></div><span id="60723"></span><div id="comment-60723" class="comment"><div id="post-60723-score" class="comment-score">1</div><div class="comment-text"><p>I've changed the diagram so that it now contains the Wireshark settings. You have to choose the correct "Stream" and you may also have to "Switch Direction" in order to see the client's packets.</p><p>Window Scaling is enabled, so no tuning needed. The "halving" is normal TCP "Congestion Avoidance" behaviour.</p><p>The throughput rate appears to be the "fault" of the sending application rather than anything at the TCP/IP or network level.</p><p>The reason that the sending client is behaving this way is not apparent from the capture. You're going to have to do some different analysis inside your virtual machine and/or application. Is there a buffer limitation for the application to deliver data to TCP?</p><p>Those slow output rates, such as at time 11:38 on the chart, are because the sender is choosing to do that, not because of any network or TCP/IP limitation.</p></div><div id="comment-60723-info" class="comment-info"><span class="comment-age">(10 Apr '17, 17:12)</span> Philst</div></div></div><div id="comment-tools-60699" class="comment-tools"></div><div class="clear"></div><div id="comment-60699-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<div class="paginator-container-left">

</div>

</div>

</div>


+++
type = "question"
title = "Editcap [and tshark] performance"
description = '''I have a 9.59GB pcap that I am running editcap -D from 1 to 1000000 exponentially. There are 13751611 packets in the file. I have a dedicated Win 7 Ent. VM on an ESXi server with 26GB RAM with 2 dual-core 3GHz CPUs. It took 234991 seconds to process the command with a window of 1000000. The command ...'''
date = "2013-10-08T14:50:00Z"
lastmod = "2013-10-09T13:26:00Z"
weight = 25765
keywords = [ "performance" ]
aliases = [ "/questions/25765" ]
osqa_answers = 2
osqa_accepted = false
+++

<div class="headNormal">

# [Editcap \[and tshark\] performance](/questions/25765/editcap-and-tshark-performance)

</div>

<div id="main-body">

<div id="askform">

<table id="question-table" style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><span id="post-25765-upvote" class="ajax-command post-vote up" rel="nofollow" title="I like this post (click again to cancel)"> </span><div id="post-25765-score" class="post-score" title="current number of votes">0</div><span id="post-25765-downvote" class="ajax-command post-vote down" rel="nofollow" title="I dont like this post (click again to cancel)"> </span> <span id="favorite-mark" class="ajax-command favorite-mark" rel="nofollow" title="mark/unmark this question as favorite (click again to cancel)"> </span><div id="favorite-count" class="favorite-count"></div></div></td><td><div id="item-right"><div class="question-body"><p>I have a 9.59GB pcap that I am running editcap -D from 1 to 1000000 exponentially. There are 13751611 packets in the file. I have a dedicated Win 7 Ent. VM on an ESXi server with 26GB RAM with 2 dual-core 3GHz CPUs. It took 234991 seconds to process the command with a window of 1000000. The command took only 1.5 GB of RAM and the CPUs didn't seem very taxed. Is there a way to get this to finish faster?</p><p>I realize that BBP is to cut the files into smaller chunks when analyzing them for better speeds, but if I do it breaks the coherence of the -D window.</p></div><div id="question-tags" class="tags-container tags"><span class="post-tag tag-link-performance" rel="tag" title="see questions tagged &#39;performance&#39;">performance</span></div><div id="question-controls" class="post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>asked <strong>08 Oct '13, 14:50</strong></p><img src="https://secure.gravatar.com/avatar/9231d57e09cb52e00e39dede07ab6ad3?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="karl&#39;s gravatar image" /><p><span>karl</span><br />
<span class="score" title="16 reputation points">16</span><span title="2 badges"><span class="badge1">●</span><span class="badgecount">2</span></span><span title="2 badges"><span class="silver">●</span><span class="badgecount">2</span></span><span title="5 badges"><span class="bronze">●</span><span class="badgecount">5</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="karl has no accepted answers">0%</span></p></div><div class="post-update-info post-update-info-edited"><p><span> edited <strong>08 Oct '13, 14:51</strong> </span></p></div></div><div id="comments-container-25765" class="comments-container"></div><div id="comment-tools-25765" class="comment-tools"></div><div class="clear"></div><div id="comment-25765-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

------------------------------------------------------------------------

<div class="tabBar">

<span id="sort-top"></span>

<div class="headQuestions">

2 Answers:

</div>

</div>

<span id="25766"></span>

<div id="answer-container-25766" class="answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><span id="post-25766-upvote" class="ajax-command post-vote up" rel="nofollow" title="I like this post (click again to cancel)"> </span><div id="post-25766-score" class="post-score" title="current number of votes">1</div><span id="post-25766-downvote" class="ajax-command post-vote down" rel="nofollow" title="I dont like this post (click again to cancel)"> </span></div></td><td><div class="item-right"><div class="answer-body"><p>Is there a reason why you use a window of 1 million frames? If you're deduplicating frames that are a result of multi SPAN sources you should be getting good to perfect results with windows a fraction that size - I'm using 100 myself quite often, and it almost never fails. Duplicates that editcap removes are appearing within a couple of milliseconds at most, often single digit microseconds - it makes no sense to waste performance on a huge window of 1 million frames.</p><p>I don't think RAM size and CPU are the problem, it's probably more the disk I/O and the searching in the huge list of MD5 hashes that takes the longest time. If I were you I'd do a test run with a window of 100 frames to see how fast it performs, maybe on a smaller trace at first.</p><p>Update: wait, what do you mean by "running exponentially"? Are you saying that you start with -D 1, then -D 2, -D 3, until -D 1000000? If so: seriously? why would you do that?</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>08 Oct '13, 15:28</strong></p><img src="https://secure.gravatar.com/avatar/c578ba2967741f25aebd6afef702f432?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="Jasper&#39;s gravatar image" /><p><span>Jasper ♦♦</span><br />
<span class="score" title="23806 reputation points"><span>23.8k</span></span><span title="5 badges"><span class="badge1">●</span><span class="badgecount">5</span></span><span title="51 badges"><span class="silver">●</span><span class="badgecount">51</span></span><span title="284 badges"><span class="bronze">●</span><span class="badgecount">284</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="Jasper has 263 accepted answers">18%</span></p></div><div class="post-update-info post-update-info-edited"><p><span> edited <strong>08 Oct '13, 15:30</strong> </span></p></div></div><div id="comments-container-25766" class="comments-container"><span id="25770"></span><div id="comment-25770" class="comment"><div id="post-25770-score" class="comment-score"></div><div class="comment-text"><p>I am comparing the removal behavior of editcap -D.</p><p>By "running exponentially" I mean -D 1, -D 2, -D 3, -D 5, -D 6, -D 10, -D 17, -D 31, -D 56, -D 100, -D 177, -D 316, -D 562, -D 1000, ..., -D 562341, -D 1000000.</p><p>With this when I chart the plot on a log scale I have discretionary points early in the beginning, and ramp up exponentially to have a mid point and two points near it (one to the left and one to the right) in addition to the major magnitudes.</p><p>I have run the test on 3 other files removed pkts are shown. 93578 pkts, 55MB = default 153 (0.12%), max 3412 (3.65%) 578197 pkts, 340MB = default 2736 (0.47%), max 21036 (3.64%) 1393760 pkts, 948MB = default 6126 (0.44%), max 14329 (1.03%) This file: 13751611 pkts, 9.59GB = default 61701 (0.45%), max 129128 (0.94%)</p><p>The resultant curves are not purely linear, but have steps/ramps.</p><p>Why would there be any differences between two magnitudinally equal gigantic window sizes?</p></div><div id="comment-25770-info" class="comment-info"><span class="comment-age">(08 Oct '13, 16:17)</span> <span class="comment-user userinfo">karl</span></div></div><span id="25772"></span><div id="comment-25772" class="comment"><div id="post-25772-score" class="comment-score"></div><div class="comment-text"><blockquote><p>Why would there be any differences between two magnitudinally equal gigantic window sizes?</p></blockquote><p>Different frame sizes will take different times for the MD5 hash calculation.</p></div><div id="comment-25772-info" class="comment-info"><span class="comment-age">(08 Oct '13, 16:44)</span> <span class="comment-user userinfo">Kurt Knochner ♦</span></div></div><span id="25773"></span><div id="comment-25773" class="comment"><div id="post-25773-score" class="comment-score"></div><div class="comment-text"><p>Sorry, I mean differences in packets removed.</p></div><div id="comment-25773-info" class="comment-info"><span class="comment-age">(08 Oct '13, 16:47)</span> <span class="comment-user userinfo">karl</span></div></div><span id="25774"></span><div id="comment-25774" class="comment"><div id="post-25774-score" class="comment-score"></div><div class="comment-text"><p>What do you mean? I assume the capture files are different !?</p></div><div id="comment-25774-info" class="comment-info"><span class="comment-age">(08 Oct '13, 17:13)</span> <span class="comment-user userinfo">Kurt Knochner ♦</span></div></div><span id="25776"></span><div id="comment-25776" class="comment not_top_scorer"><div id="post-25776-score" class="comment-score"></div><div class="comment-text"><p>I have ran -D 100000 and had 128,887 pkts removed and then -D 177827 and had 128,987 pkts removed. By extending the window size by 77,827 the 100 pkts were removed. I wouldn't suspect any packets would be removed at this point, but they are. Is there a way to get a feel for how long 177827 packets are in seconds in relation to my capture?</p></div><div id="comment-25776-info" class="comment-info"><span class="comment-age">(08 Oct '13, 17:41)</span> <span class="comment-user userinfo">karl</span></div></div><span id="25777"></span><div id="comment-25777" class="comment not_top_scorer"><div id="post-25777-score" class="comment-score"></div><div class="comment-text"><blockquote><p>Is there a way to get a feel for how long 177827 packets are in seconds in relation to my capture?</p></blockquote><p>Just look at the time stamps. Select one frame, set a time reference (CTRL-T), then scroll forward 177827 frames and check the delta in the time column.</p></div><div id="comment-25777-info" class="comment-info"><span class="comment-age">(08 Oct '13, 17:52)</span> <span class="comment-user userinfo">Kurt Knochner ♦</span></div></div><span id="25779"></span><div id="comment-25779" class="comment"><div id="post-25779-score" class="comment-score">1</div><div class="comment-text"><p>editcap -D may remove additional frames that are very distant to each other but not duplicates (false positives). A common example are BPDU frames - if the Spanning Tree is stable (and it should be) ALL BPDU frames are bitwise identical, but 3 seconds apart. It is quite possible that with huge ranges like yours you'll detect them as duplicates at some point (3 seconds being an eternity in networks, but with your range you'll eventually find things like that).</p></div><div id="comment-25779-info" class="comment-info"><span class="comment-age">(08 Oct '13, 23:24)</span> <span class="comment-user userinfo">Jasper ♦♦</span></div></div><span id="25784"></span><div id="comment-25784" class="comment not_top_scorer"><div id="post-25784-score" class="comment-score"></div><div class="comment-text"><blockquote><p>ALL BPDU frames are bitwise identical, but 3 seconds apart. It is quite possible that with huge ranges like yours you'll detect them as duplicates at some point</p></blockquote><p>good one! +1</p></div><div id="comment-25784-info" class="comment-info"><span class="comment-age">(09 Oct '13, 01:46)</span> <span class="comment-user userinfo">Kurt Knochner ♦</span></div></div><span id="25843"></span><div id="comment-25843" class="comment not_top_scorer"><div id="post-25843-score" class="comment-score"></div><div class="comment-text"><p>Is there a way to put the duplicate packets in their own file to inspect them?</p></div><div id="comment-25843-info" class="comment-info"><span class="comment-age">(09 Oct '13, 10:23)</span> <span class="comment-user userinfo">karl</span></div></div></div><div id="comment-tools-25766" class="comment-tools"><span class="comments-showing"> showing 5 of 9 </span> <a href="#" class="show-all-comments-link">show 4 more comments</a></div><div class="clear"></div><div id="comment-25766-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<span id="25771"></span>

<div id="answer-container-25771" class="answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><span id="post-25771-upvote" class="ajax-command post-vote up" rel="nofollow" title="I like this post (click again to cancel)"> </span><div id="post-25771-score" class="post-score" title="current number of votes">0</div><span id="post-25771-downvote" class="ajax-command post-vote down" rel="nofollow" title="I dont like this post (click again to cancel)"> </span></div></td><td><div class="item-right"><div class="answer-body"><p>From the <a href="http://www.wireshark.org/docs/man-pages/editcap.html">editcap man page</a></p><pre><code>NOTE: Specifying large &lt;dup time=&quot;&quot; window=&quot;&quot;&gt; values with large tracefiles can result in very long processing times for editcap.</code></pre><p>I guess you hit that constraint ;-)</p><blockquote><p>I am running editcap -D from 1 to 1000000 exponentially. There are 13751611 packets in the file</p></blockquote><p>This will end up in 13751611 MD5 calculations <strong>and</strong> roughly 13751611 * 1000000 MD5 comparisons (minus a few at the beginning because there are less MD5 sums than the window size). The later (MD5 sum comparisons) will kill your execution time.</p><blockquote><p>Is there a way to get this to finish faster?</p></blockquote><p>Try to narrow down the window and then run editcap with the 'optimized' window size.</p><p>Here is how I would do it.</p><ul><li>Read the pcap file with a script (like Perl Net::Pcap or Python Scapy)</li><li>Create a MD5 sum of each packet (full bytes!)</li><li>Print: 'MD5 hash'; 'Frame number'</li><li>At the end of the pcap file: Sort the output (MD5 hashes)</li><li>Read the sorted output and try to find two consecutive identical (duplicate) MD5 hashes. If you find one: Subtract the frame numbers and store that as max window. Repeat this step until the end. If you find a larger max window, replace the old value with the new value.</li><li>At the end: Print the max window value</li></ul><p>Take the <strong>max window</strong> value of your script, add 15% (just to be safe ;-)) and use that value as an input for editcap -D. BTW: If your max window is 0 (you did not find any duplicate MD5 hashes), you can skip editcap, as there are no duplicates.<br />
</p><p>I <strong>guess</strong> that this will be faster, because there are a ton less comparisons to make, however I have no prove (yet) :-) <strong>However:</strong> Maybe the sort operation will be heavy as well (or heavy as hell). I'm running some test right now ;-)</p><p><strong>++ UPDATE ++</strong></p><p>O.K. you can ignore the sort operation. I created 100.000 fake frames of 1500 bytes length, calculated the MD5 sum and then sorted the sums.</p><p><strong>Result:</strong> 100.000 Frames</p><p>Create (/dev/urandom) and calculate MD5 sum: <strong>6m37.485s</strong><br />
Sort the MD5 sums: real <strong>0m0.186s</strong></p><p>So, sorting is ways faster than creating the MD5 sums. Who had thought of that? ;-)</p><p>My test was done in a VM on a latop and it took ~400 seconds for 100.000 frames. So it will take ~137 times longer for your capture file, which is ~55.000 seconds. Although this is much faster than your time, it is still 15 hours (mostly MD5 calculation)!! However your server is probably faster than my laptop.</p><p><strong>Pros and Cons:</strong></p><p><strong>Pro:</strong> If you're lucky, there are no duplicate MD5 sums, so you're done after the first step.<br />
<strong>Con:</strong> If things went wrong, you'll find the max window to be 1000000 (max of editcap), and then you'll have to run editcap on top, which means you just lost the 15 hours needed for pre-processing. However, how likely is it to have duplicates within a range of 1000000 packets ?!?</p><p>I guess that in real world scenarios you will find the max windows between 100 and 1000 as <span>@Jasper</span> also mentioned. So, if you want the 'exact' result, you can use my pre-processing method to speed up things. Otherwise, just use a window of 1000+ and rely on the 'rule of thumb' ;-)</p><p>Regards<br />
Kurt</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>08 Oct '13, 16:29</strong></p><img src="https://secure.gravatar.com/avatar/23b7bf5b13bc2c98b2e8aa9869ca5d75?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="Kurt%20Knochner&#39;s gravatar image" /><p><span>Kurt Knochner ♦</span><br />
<span class="score" title="24767 reputation points"><span>24.8k</span></span><span title="10 badges"><span class="badge1">●</span><span class="badgecount">10</span></span><span title="39 badges"><span class="silver">●</span><span class="badgecount">39</span></span><span title="237 badges"><span class="bronze">●</span><span class="badgecount">237</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="Kurt Knochner has 344 accepted answers">15%</span> </br></br></p></div></div><div id="comments-container-25771" class="comments-container"><span id="25775"></span><div id="comment-25775" class="comment"><div id="post-25775-score" class="comment-score"></div><div class="comment-text"><p>If all duplicate packet numbers can be deduced this way, then can these numbers be passed to editcap as parameters and the resultant files could be created quicker?</p></div><div id="comment-25775-info" class="comment-info"><span class="comment-age">(08 Oct '13, 17:33)</span> <span class="comment-user userinfo">karl</span></div></div><span id="25778"></span><div id="comment-25778" class="comment"><div id="post-25778-score" class="comment-score"></div><div class="comment-text"><p>well ... no, because you obviously have way to many packets to remove (you mention 128987 in one comment). You simply cannot give that many options (frame numbers) to editcap.</p><p>Anyway, currently I don't get what you are trying to do.</p><ul><li><p>Why do you see that many duplicate frames with such a large gap between them (177827 frames)?</p></li><li><p>What kind of network are we talking about (1G/s, 10G/s, 40G/s)?</p></li><li><p>How do you capture?<br />
</p></li><li><p>Why do you need to eliminate duplicate frames?</p></li><li><p>Why did you run editcap "exponentially"? Is this just for fun, or a real world problem?</p></li></ul></div><div id="comment-25778-info" class="comment-info"><span class="comment-age">(08 Oct '13, 17:56)</span> <span class="comment-user userinfo">Kurt Knochner ♦</span></div></div><span id="25842"></span><div id="comment-25842" class="comment"><div id="post-25842-score" class="comment-score"></div><div class="comment-text"><p>I don't know why there are still duplicates with window size of 177827.</p><p>The network is a lot slower. See capinfos from the original file: File type: Wireshark - pcapng File encapsulation: Ethernet File size: 10305318996 bytes Data size: 9848966269 bytes Capture duration: Data byte rate: 30071.70 bytes/sec Data bit rate: 240573.61 bits/sec Average packet size: 716.20 bytes Average packet rate: 41.99 packets/sec</p><p>This implies the window (177827) is on average 1 hour 10 minutes and 35 seconds wide.</p><p>The capture was on a mirror port of a switch. A router took all users and then sent it over our [slow] WAN link.</p><p>I am eliminated duplicates because a "pro" said to.</p><p>I am running exponentially to prove <span>@Jasper</span>, your, mine, and the man pages expressed feeling of the rules of thumb. It's real world only for the sake of proving the capabilities of editcap -D. It's fun because it's a simple test... just super long.</p><p>My problem is that editcap doesn't seem to allocate memory correctly when memory is available.</p></div><div id="comment-25842-info" class="comment-info"><span class="comment-age">(09 Oct '13, 10:20)</span> <span class="comment-user userinfo">karl</span></div></div><span id="25845"></span><div id="comment-25845" class="comment"><div id="post-25845-score" class="comment-score"></div><div class="comment-text"><blockquote><p>I don't know why there are still duplicates with window size of 177827.</p></blockquote><p>because there are a lot of possible 'candidates' if you look back that far.</p><ul><li><strong>ARP requests</strong> for the same address are identical</li><li>Cisco <strong>CDP</strong> frames are identical</li><li>Spanning Tree <strong>BPDUs</strong> are identical</li><li>many other broadcast frames are identical</li></ul><p>So, if you just go back far enough, you will <strong>always</strong> find some duplicates! But they are duplicate by nature and not due to an error on the network.</p><blockquote><p>This implies the window (177827) is on average 1 hour 10 minutes and 35 seconds wide.</p></blockquote><p>It really, really does not make sense to search for duplicates in that time window. Where should the real duplicate frames come from? Did circulate in a dark area of the network, just to pop out an hour later? Nah...</p><blockquote><p>I am eliminated duplicates because a "pro" said to.</p></blockquote><p>Greetings to your "pro".</p><p>If I was a "pro", I would eliminate duplicate frames only if I had a problem during analysis with them and not as a precaution. It costs time, it might cause confusion (sounds familiar ;-)), etc.</p><blockquote><p>My problem is that <strong>editcap doesn't seem to allocate memory correctly</strong> when memory is available.</p></blockquote><p>How do you get to this conclusion? editcap 'allocates' the memory for the max amount of MD5 hashes as a static data structure and then only a <strong>few</strong> more things dynamically. Why does it not allocate more memory? It does not need it. See the code.</p><p><strong>My overall recommendation:</strong> Simply stop doing, what you are doing, as it does not give you any real benefit, <strong>unless</strong> you have a real problem during the analysis phase with <strong>real</strong> duplicate frames. In contrary, it leads to massive confusion as we have seen in this discussion about phantom duplicate frames ;-)) Nevertheless, I thank you for asking this question, as I had a reason to check the editcap code and now I understand how that stuff works ;-))</p></div><div id="comment-25845-info" class="comment-info"><span class="comment-age">(09 Oct '13, 13:26)</span> <span class="comment-user userinfo">Kurt Knochner ♦</span></div></div></div><div id="comment-tools-25771" class="comment-tools"></div><div class="clear"></div><div id="comment-25771-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<div class="paginator-container-left">

</div>

</div>

</div>


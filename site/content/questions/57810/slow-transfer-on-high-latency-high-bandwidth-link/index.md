+++
type = "question"
title = "Slow Transfer on high latency high bandwidth link"
description = '''The upload made from a client (win 7) has very poor performance. The same made from another client has acceptable performance.  slow upload -&amp;gt; slow_session_anon.pcapng fast upload -&amp;gt; fast_session_anon.pcapng http://en.file-upload.net/download-12139835/captures.rar.html When I compared both fil...'''
date = "2016-12-03T01:57:00Z"
lastmod = "2016-12-07T01:22:00Z"
weight = 57810
keywords = [ "slow", "upload" ]
aliases = [ "/questions/57810" ]
osqa_answers = 2
osqa_accepted = true
+++

<div class="headNormal">

# [Slow Transfer on high latency high bandwidth link](/questions/57810/slow-transfer-on-high-latency-high-bandwidth-link)

</div>

<div id="main-body">

<div id="askform">

<table id="question-table" style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><div id="post-57810-score" class="post-score" title="current number of votes">1</div><div id="favorite-count" class="favorite-count">1</div></div></td><td><div id="item-right"><div class="question-body"><p>The upload made from a client (win 7) has very poor performance. The same made from another client has acceptable performance.</p><p>slow upload -&gt; slow_session_anon.pcapng fast upload -&gt; fast_session_anon.pcapng <a href="http://en.file-upload.net/download-12139835/captures.rar.html">http://en.file-upload.net/download-12139835/captures.rar.html</a></p><p>When I compared both files, i could see that the slow upload does not enter slow start phase. it constantly transfers at the same rate. Please comment on my thoughts and potential reason for such an issue.</p><p>Thanks</p></div><div id="question-tags" class="tags-container tags">slow upload</div><div id="question-controls" class="post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>asked <strong>03 Dec '16, 01:57</strong></p><img src="https://secure.gravatar.com/avatar/5de3f05c3183608f6986dd68fa7eb0f3?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="soochi&#39;s gravatar image" /><p>soochi<br />
<span class="score" title="57 reputation points">57</span><span title="3 badges"><span class="badge1">●</span><span class="badgecount">3</span></span><span title="4 badges"><span class="silver">●</span><span class="badgecount">4</span></span><span title="9 badges"><span class="bronze">●</span><span class="badgecount">9</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="soochi has no accepted answers">0%</span></p></div></div><div id="comments-container-57810" class="comments-container"><span id="57811"></span><div id="comment-57811" class="comment"><div id="post-57811-score" class="comment-score"></div><div class="comment-text"><p>Could you please try google drive cloudshark or dropbox. Can´ find the correct download link at the site. If I press download I should download a really strange executable. Which I will not do.</p></div><div id="comment-57811-info" class="comment-info"><span class="comment-age">(03 Dec '16, 02:06)</span> Christian_R</div></div><span id="57815"></span><div id="comment-57815" class="comment"><div id="post-57815-score" class="comment-score"></div><div class="comment-text"><p><a href="https://drive.google.com/file/d/0B7Io9WiIN49VU3F2Z1VKbUwxVVk/view?usp=sharing">https://drive.google.com/file/d/0B7Io9WiIN49VU3F2Z1VKbUwxVVk/view?usp=sharing</a></p></div><div id="comment-57815-info" class="comment-info"><span class="comment-age">(03 Dec '16, 03:55)</span> soochi</div></div><span id="57818"></span><div id="comment-57818" class="comment"><div id="post-57818-score" class="comment-score"></div><div class="comment-text"><p>For me at the first view the traces loks more or less similar. From my point of view: At least 1 of the next points is needed here for more detailed analysis. 1. The packets are truncated to 60 Bytes so the SMB part is missing. 2. Both traces are local traces with segementation offload enabled, so we can´ see what happens at the wire.</p><p>Either you disable segemntation offload during capture -&gt; But it is also only a workaround. Or you try to capture as close as possible outside to that machine. Maybe with TAPs or SPAN ports.</p></div><div id="comment-57818-info" class="comment-info"><span class="comment-age">(03 Dec '16, 09:39)</span> Christian_R</div></div><span id="57826"></span><div id="comment-57826" class="comment"><div id="post-57826-score" class="comment-score"></div><div class="comment-text"><p>The payload after TCP is encrypted, so only until TCP could be interpreted. Yes both captures are made at the end hosts which has offloaded segmentation to NIC.</p><p>But still analysis must be possible because: 1, both traces are identical until frame number 29 (which ACKs frame 25) 2, the slow transfer total duration until this point is 229ms and fast transfer is 240ms</p><p>There is something which causes the slow transfer to always send segments of 4133 and 37 bytes (stays constantly). what could be the cause for that? considering both the transfers uses the same network path and same network conditions what could be the reason for such a packet pattern.</p><p>What could cause an application to send data in such a fashion.</p></div><div id="comment-57826-info" class="comment-info"><span class="comment-age">(04 Dec '16, 02:15)</span> soochi</div></div><span id="57827"></span><div id="comment-57827" class="comment"><div id="post-57827-score" class="comment-score"></div><div class="comment-text"><p>Yes you are right with the application layer. I have thought we talk about smb, but seems that mixed it wit an other question here. I will have a deeper looker today.</p></div><div id="comment-57827-info" class="comment-info"><span class="comment-age">(04 Dec '16, 02:46)</span> Christian_R</div></div><span id="57831"></span><div id="comment-57831" class="comment not_top_scorer"><div id="post-57831-score" class="comment-score"></div><div class="comment-text"><p>First of all you are right, the fast transfer is 10 times faster, than the slower. But with this tcp segemtation offloading the root cause is still hard to find.</p></div><div id="comment-57831-info" class="comment-info"><span class="comment-age">(04 Dec '16, 05:52)</span> Christian_R</div></div><span id="57832"></span><div id="comment-57832" class="comment not_top_scorer"><div id="post-57832-score" class="comment-score"></div><div class="comment-text"><p>It seems that the client app does send the data to the stack in a slower way, than in the fast example.</p></div><div id="comment-57832-info" class="comment-info"><span class="comment-age">(04 Dec '16, 05:59)</span> Christian_R</div></div><span id="57833"></span><div id="comment-57833" class="comment not_top_scorer"><div id="post-57833-score" class="comment-score"></div><div class="comment-text"><p>Thanks for your effort :) and you too are right regarding offloading. This is the capture which i got from the client. I will try to get a better capture from somewhere in the transit path as near as possible to the client.</p><p>But for the mean time do you find anything like TCP stack not functioning properly?</p><p>it seems to me that the TCP stack is handled data to .....</p><p>just got a mail that u worte the same :)</p></div><div id="comment-57833-info" class="comment-info"><span class="comment-age">(04 Dec '16, 06:01)</span> soochi</div></div><span id="57835"></span><div id="comment-57835" class="comment not_top_scorer"><div id="post-57835-score" class="comment-score"></div><div class="comment-text"><p>This is also what I think that the TCP stack is starving for data. But interestingly in the following combination the transfer is fast.</p><p>client -&gt; sends data to proxy (to reach proxy the latency is less than 1ms). In this constellation the transfer is fast.</p><p>I do accept that these are two different TCP connections, the one from client to server via proxy and the one direct. They do not have anything to do with each other. But in fact the slow client performs well when it connects via proxy.</p><p>because of this, i doubt if the capture from the wire would help. Have you heard of any reason for this particular situation: client app does not send data to TCP because it somehow detects that its connected via a high latency link. to be specific the client took 404us at frame 34 to send data and in fast transfer the same took just 71us and has larger amount of data.</p></div><div id="comment-57835-info" class="comment-info"><span class="comment-age">(04 Dec '16, 06:16)</span> soochi</div></div><span id="57841"></span><div id="comment-57841" class="comment not_top_scorer"><div id="post-57841-score" class="comment-score"></div><div class="comment-text"><p>I could manage the same transfer after disabling offloading <a href="https://drive.google.com/file/d/0B7Io9WiIN49VWHp3SHZoYkR6UkU/view?usp=sharing">https://drive.google.com/file/d/0B7Io9WiIN49VWHp3SHZoYkR6UkU/view?usp=sharing</a></p></div><div id="comment-57841-info" class="comment-info"><span class="comment-age">(04 Dec '16, 09:54)</span> soochi</div></div><span id="57843"></span><div id="comment-57843" class="comment not_top_scorer"><div id="post-57843-score" class="comment-score"></div><div class="comment-text"><p>Seems the problem is gone now?! The session lives now for 24 sec. like the fast one in the prevs captures! Maybe the system has problems with offloading?! If thats the case I would recommend to update the nic drivers at first.</p></div><div id="comment-57843-info" class="comment-info"><span class="comment-age">(04 Dec '16, 10:52)</span> Christian_R</div></div><span id="57844"></span><div id="comment-57844" class="comment not_top_scorer"><div id="post-57844-score" class="comment-score"></div><div class="comment-text"><p>:) no the problem exists. The transfer is only around 11 MB instead of ~70MB from the previous captures.</p></div><div id="comment-57844-info" class="comment-info"><span class="comment-age">(04 Dec '16, 10:56)</span> soochi</div></div><span id="57846"></span><div id="comment-57846" class="comment not_top_scorer"><div id="post-57846-score" class="comment-score"></div><div class="comment-text"><p>I have the following observation:</p><p>As you also told before the application does not feed TCP with data but under which circumstances does this happen... it seems to me like that when once the bytes in fly are reaching nearly the receive window, the slow stop phase stops.</p><p>to be specific at frame 45 this happens and after that the pattern of 9 frames persists consistently (slow_session.offdis_anon.pcapng).</p><p>just my thoughts please comment.</p></div><div id="comment-57846-info" class="comment-info"><span class="comment-age">(04 Dec '16, 11:40)</span> soochi</div></div><span id="57847"></span><div id="comment-57847" class="comment not_top_scorer"><div id="post-57847-score" class="comment-score"></div><div class="comment-text"><p>The question to me is: Is the fast client always fast and the slow client is always slow? If the answer is yes you could do the following things:</p><ol><li><p>Try a tool like <a href="https://github.com/ChristianRe/CRWinNetDiag">crwinnetdiag</a> to examine the differnet configurations of the clients</p></li><li><p>You can follow the answer of @mrEEde</p></li></ol><p>If the answer is no:</p><p>The problem is inside the client aplication.</p></div><div id="comment-57847-info" class="comment-info"><span class="comment-age">(04 Dec '16, 12:05)</span> Christian_R</div></div><span id="57849"></span><div id="comment-57849" class="comment not_top_scorer"><div id="post-57849-score" class="comment-score"></div><div class="comment-text"><p>I will verify/double check if both the clients have the same config.</p><p>ultimately my goal is to explain this strange behavior. There must be some factor triggering this condition. What do you think of my previous comments?</p></div><div id="comment-57849-info" class="comment-info"><span class="comment-age">(04 Dec '16, 12:31)</span> soochi</div></div><span id="57927"></span><div id="comment-57927" class="comment not_top_scorer"><div id="post-57927-score" class="comment-score"></div><div class="comment-text"><p>I suggest you to examine both clients with crwinnetdiag, because it parses the relevant Registry settings and it should be easy to find the differences with that tool.</p><p>And it also parses the AFD Registry settings.</p></div><div id="comment-57927-info" class="comment-info"><span class="comment-age">(07 Dec '16, 03:29)</span> Christian_R</div></div></div><div id="comment-tools-57810" class="comment-tools"><span class="comments-showing"> showing 5 of 16 </span> <a href="#" class="show-all-comments-link">show 11 more comments</a></div><div class="clear"></div><div id="comment-57810-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

------------------------------------------------------------------------

<div class="tabBar">

<span id="sort-top"></span>

<div class="headQuestions">

2 Answers:

</div>

</div>

<span id="57921"></span>

<div id="answer-container-57921" class="answer accepted-answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><div id="post-57921-score" class="post-score" title="current number of votes">2</div></div></td><td><div class="item-right"><div class="answer-body"><p>First some items that are the same (or close) in both captures:</p><p>3-way handshakes are the same, client supports windows scaling but server does not (or something in the middle doesn't). Therefore, the receive window can never be more than 64KB. Both support SACKs and both specify an MSS=1460. The server advertises a receive window of 64KB constantly in both cases (after a ramp-up very early on that closely matches the slow start). The 64KB is reached by ACK #101 in "fast" and ACK #103 in "slow". The minimum RTT is the same in both cases at just under 22ms (the "fast" example is a touch longer than the "slow").</p><p>There are several instances of packets arriving out-of-order at the receiver (but in the correct order when we see them). The amount of time they are OOO is just 1 microsecond. We can infer this by SACKs that originate from the receiver. I'll just give one example from each capture:</p><p>"Slow": The first 1460 bytes of #47602 are overtaken downstream. One RTT later, SACKs #47613, #47614, #47615 announce a gap of 1460 then ACK #47616 (zero time after #47615) tells us that the 1460 byes arrived after all. That is not enough to stop the client sending #47617, a retransmission of the 1460 bytes, in response to the 3 x SACKs (equals 3 x Dup-ACKs).</p><p>"Fast": Same story. Data in #33247 is overtaken, SACKs #33321, #33322, #33323 announce it, ACK #33324 acknowledges the missing 1460 but client data #33325 is sent as an unnecessary retransmission.</p><p>These OOOs and retransmissions have very little or no effect on the overall throughput.</p><p>The difference in throughputs is due to the different send buffer sizes. "Fast" maintains a consistent packets-in-flight of 64KB whereas "slow" maintains just 8KB. You could calculate 64KB per RTT vs 8KB per RTT (RTT=22ms) - or just say that "fast" has 8 times the throughput of "slow".</p><p>Compare these snippets of Stream-TCP-Trace graphs - which are essentially the same for the whole periods. Note the vertical height of the bursts per RTT.</p><p><img src="https://osqa-ask.wireshark.org/upfiles/Slow.PNG" alt="Slow" /> <img src="https://osqa-ask.wireshark.org/upfiles/Fast.PNG" alt="Fast" /></p><p>But what is the cause?</p><p>In the "slow" example, we see one of those OOO cases very early, during the third burst in the slow start phase when we've only ramped up to 7KB per RT. This behaviour is not in the "fast" capture.</p><p>Packet #30 is seen in the correct order in the capture - but is overtaken before it arrives at the receiver. This can be inferred by the SACKs #36 and #37 followed by ACK #38, all in the space of 2 microseconds. There is no data retransmission because we only got two SACKs (which are also treated as Dup-ACKs) - not the three required for a fast retransmission.</p><p>With only one example of each, I can only guess why the client decides to keep its send buffer at just 8KB. It seems unlikely that the two SACKs are enough to "scare" it into the consistent reduced throughput for the whole duration? The timing of the OOO may be just unlucky.</p><p>If the same PC is "slow" every single time, then it would seem more likely that some registry settings related to send buffer size are different.</p><p>A quick Google gave me this (from "social.technet.microsoft.com/Forums"):</p><p><em>Looks like afd.sys of windows 7 uses new registry key named HKLM\SYSTEM\CurrentControlSet\Services\AFD\Parameters\DynamicSendBufferDisable (DWORD). Setting it to zero forces winsock to use larger buffers.</em></p><p>This is just one possibility. There may be a variety of other registry entries to find. There also may be other reasons for the small send buffer in this PC.</p><p><strong>Update:</strong></p><p>More Googling has revealed that this problem was well known. There are many web articles about this Windows 7 "flaw".</p><hr /><p><strong>A good one I found is from Feb 2009: (kb.pert.geant.net/PERTKB/WindowsOSSpecific)</strong></p><hr /><p>Buffers for TCP Send Windows</p><p>Inquiry at Microsoft (thanks to Larry Dunn) has revealed that the default send window is 8KB and that there is no official support for configuring a system-wide default.</p><p>However, the current Winsock implementation uses the following undocumented registry key for this purpose</p><p>[HKEY_LOCAL_MACHINE\System\CurrentControlSet\Services\AFD\Parameters]</p><p>Value Name: DefaultSendWindow</p><p>Data Type: REG_DWORD (DWORD Value)</p><p>Value Data: The window size in bytes. The maximum value is unknown.</p><p>According to Microsoft, this parameter may not be supported in future Winsock releases. However, this parameter is confirmed to be working with Windows XP, Windows 2000, Windows Vista and even the Windows 7 Beta 1.</p><p>This value needs to be manually adjusted (e.g., DrTCP can't do it), or the application needs to set it (e.g., iperf with '-l' option).</p><p>It may be difficult to detect this as a bottleneck because the host will advertise large windows but will only have about 8.5KB of data in flight.</p><hr /><p><strong>Another useful one is: (www.htpcforums.com/index.php?showtopic=631)</strong></p><hr /><p>The problem was that I could download fine but my upload maxed out around 2 Mbps! It was 10x slower than it should have been.</p><p>Then I learned about DefaultSendWindow and the fact that it isn't properly set on Vista machines but it is set in Windows XP, believe it or not. For a while I was wondering why my Windows XP laptop could do 10 Mbps up over WiFi while my wired desktop could only do 2 Mbps.</p><p>In both cases, the DefaultSendWindow needed to be changed, here's how:</p><p>1) Regedit:</p><p>HKLM\System\CurrentControlSet\Services\AFD\Parameters</p><p>2) If the DefaultSendWindow value is not in there, you will need to add it. Right-click on the right column and choose "New &gt; DWORD Value (32-bit)" and name it DefaultSendWindow. Make sure that you enter it exactly like that, with the first letter of each word capitalized.</p><p><em>Note from PhilSt: Section 3 about calculating values snipped. We need 65536 here (0x10000).</em></p><p>4) Once you have the value, double-click the DefaultSendWindow value and make sure the "Base" part is on Decimal. Enter the number you got from the formula above into the value field, then put the radio button on "Hexadecimal" and click OK. The value needs to be in hexadecimal format for it to work right, but it is easier to calculate the optimal value in decimal format so that is why I said to enter it in decimal then change it to hex before saving.</p><p>4) Restart your system</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>07 Dec '16, 01:22</strong></p><img src="https://secure.gravatar.com/avatar/35a0c1d0cf15b9d54d73bf54ae28abcd?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="Philst&#39;s gravatar image" /><p>Philst<br />
<span class="score" title="431 reputation points">431</span><span title="1 badges"><span class="badge1">●</span><span class="badgecount">1</span></span><span title="6 badges"><span class="silver">●</span><span class="badgecount">6</span></span><span title="16 badges"><span class="bronze">●</span><span class="badgecount">16</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="Philst has 6 accepted answers">27%</span></p></img></div><div class="post-update-info post-update-info-edited"><p>edited 08 Dec '16, 16:41</p></div></div><div id="comments-container-57921" class="comments-container"><span id="57990"></span><div id="comment-57990" class="comment"><div id="post-57990-score" class="comment-score"></div><div class="comment-text"><p>The application send buffer was not properly implemented. This the cause of the issue. DefaultSendWindow as a workaround solved the issue.</p><p>i tried out various values for the send buffer and finally choose 64K as the optimum value. The programmer is informed of this issue.</p><p>Thanks Philst good catch :)</p></div><div id="comment-57990-info" class="comment-info"><span class="comment-age">(10 Dec '16, 03:18)</span> soochi</div></div><span id="57992"></span><div id="comment-57992" class="comment"><div id="post-57992-score" class="comment-score"></div><div class="comment-text"><p>Well I think the solution you have found is a good one. But just out of curiosity have you ever found out why the fast client is the faster one?</p></div><div id="comment-57992-info" class="comment-info"><span class="comment-age">(10 Dec '16, 10:19)</span> Christian_R</div></div><span id="58001"></span><div id="comment-58001" class="comment"><div id="post-58001-score" class="comment-score">2</div><div class="comment-text"><p>because of the registry entry (DefaultSendWindow).</p><p>This entry came into place on the faster computer due to installation of Citrix client software.</p><p>I made several verification to make sure that the Jave software is not implemented properly:</p><p>1, Used the non Java based version of the software to do the upload. With Internet explorer 49K of bytes in flight achieved. Firefox could send up to the server window size (64K), there were many events of window full condition.</p><p>2, Removed the DefaultSendWindow setting from fast computer which then could only send to a maximum bytes in fly of 8K. Configured this setting on slower computer which improved performance.</p><p>as per my tests, this setting does increase/decrease the Bytes in Flight in the case when the software is not programmed to provide "send buffer".</p></div><div id="comment-58001-info" class="comment-info"><span class="comment-age">(11 Dec '16, 02:22)</span> soochi</div></div><span id="58003"></span><div id="comment-58003" class="comment"><div id="post-58003-score" class="comment-score"></div><div class="comment-text"><p>Yes that explaines a lot of things. And than this solution is really your root cause solution.</p></div><div id="comment-58003-info" class="comment-info"><span class="comment-age">(11 Dec '16, 02:41)</span> Christian_R</div></div><span id="58007"></span><div id="comment-58007" class="comment"><div id="post-58007-score" class="comment-score"></div><div class="comment-text"><p>But one last question have you manipulated the autotuning values at win7 before?</p></div><div id="comment-58007-info" class="comment-info"><span class="comment-age">(11 Dec '16, 11:51)</span> Christian_R</div></div><span id="58029"></span><div id="comment-58029" class="comment not_top_scorer"><div id="post-58029-score" class="comment-score"></div><div class="comment-text"><p>which exact values do u mean?</p></div><div id="comment-58029-info" class="comment-info"><span class="comment-age">(12 Dec '16, 12:33)</span> soochi</div></div><span id="58030"></span><div id="comment-58030" class="comment not_top_scorer"><div id="post-58030-score" class="comment-score"></div><div class="comment-text"><p>something like this:</p><p>netsh interface tcp show global</p></div><div id="comment-58030-info" class="comment-info"><span class="comment-age">(12 Dec '16, 12:42)</span> Christian_R</div></div><span id="58033"></span><div id="comment-58033" class="comment not_top_scorer"><div id="post-58033-score" class="comment-score"></div><div class="comment-text"><p>no changes were made from the default config other than the software installation changed the default send buffer, which i did not see in any netsh output.</p></div><div id="comment-58033-info" class="comment-info"><span class="comment-age">(12 Dec '16, 14:02)</span> soochi</div></div><span id="58058"></span><div id="comment-58058" class="comment not_top_scorer"><div id="post-58058-score" class="comment-score"></div><div class="comment-text"><p>I'm glad to know that you have verified the answer and implemented a "fix".</p><p>There is room for even more improvement.</p><p>Have another look at the "fast" chart above. There are still sizable "pauses" in the flow - waiting for each 64KB chunk to traverse the network and be ACKed.</p><p>If you send even more packets per round trip, you'd get even higher throughput. With 128KB per RTT, you'd double throughput and halve the time.</p><p>To achieve this you need to enable TCP Window scaling (at both ends) - as well as set a larger DefaultSendWindow value.</p></div><div id="comment-58058-info" class="comment-info"><span class="comment-age">(13 Dec '16, 17:39)</span> Philst</div></div></div><div id="comment-tools-57921" class="comment-tools"><span class="comments-showing"> showing 5 of 9 </span> <a href="#" class="show-all-comments-link">show 4 more comments</a></div><div class="clear"></div><div id="comment-57921-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<span id="57836"></span>

<div id="answer-container-57836" class="answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><div id="post-57836-score" class="post-score" title="current number of votes">0</div></div></td><td><div class="item-right"><div class="answer-body"><p>This might be a problem with the Nagle algorithm enabled at the sender and the dataflow flow involving an odd number of segments with the last one being a non-full-MSS .<br />
Do both clients have Nagle enabled, or just one? What is the sending application?<br />
You might want to try disabling it as per <a href="http://www.optimizemswindows.com/disable-nagle-algorithm-to-increase-your-internet-speed-for-quick-response/">disabling-Nagle-in-Windows</a></p><p>Regards Matthias</p><hr /><p>Why do I think that Nagle is in place and the session is waiting for an acknowledgement ?</p><p>If you apply the filter <code>tcp.len&gt; 0 or tcp.time_delta&gt;0.019</code> to the <a href="https://drive.google.com/file/d/0B7Io9WiIN49VWHp3SHZoYkR6UkU/view?usp=sharing">non-offload trace</a> you will notice that the session is stalled whenever windows sent a sub_ MSS segment before the gap (that matches the RTT). <img src="https://osqa-ask.wireshark.org/upfiles/Selection_601.png" alt="alt text" /><br />
If you count the number of segments that were sent in the same batch until the hang occurs it is always an odd number - at its fastest transfer rate 9 segments in a row.</p><p>And odd numbers of segments in a row with the last one being less than one MSS rings a bell ...<br />
So while it is still a guess, I think we are suffering half of the problem described in <a href="https://www.youtube.com/watch?v=adDC5T-RzR4">Hangsan Bae's youtube video: TCP Nagle/Delayed ACK</a> with the missing half being the delayed ACKs.</p><p>Regards Matthias</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>04 Dec '16, 06:46</strong></p><img src="https://secure.gravatar.com/avatar/5500bd1decb766660522dfb347eedc49?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="mrEEde&#39;s gravatar image" /><p>mrEEde<br />
<span class="score" title="3892 reputation points"><span>3.9k</span></span><span title="15 badges"><span class="badge1">●</span><span class="badgecount">15</span></span><span title="22 badges"><span class="silver">●</span><span class="badgecount">22</span></span><span title="70 badges"><span class="bronze">●</span><span class="badgecount">70</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="mrEEde has 48 accepted answers">20%</span> </br></br></p></img></div><div class="post-update-info post-update-info-edited"><p>edited 04 Dec '16, 13:31</p></div></div><div id="comments-container-57836" class="comments-container"><span id="57848"></span><div id="comment-57848" class="comment"><div id="post-57848-score" class="comment-score"></div><div class="comment-text"><p>Both the clients should have the same configurations.</p><p>Can you please explain me the reason of thinking why Nagle is in place? Can that be seen in the trace or is it a just guess?</p></div><div id="comment-57848-info" class="comment-info"><span class="comment-age">(04 Dec '16, 12:28)</span> soochi</div></div><span id="57914"></span><div id="comment-57914" class="comment"><div id="post-57914-score" class="comment-score"></div><div class="comment-text"><p>I think that i get your point. Basically disabling Nagle should resolve the issue at least by half and the rest half by disabling Delayed ACK at receiver.</p><p>let me describe for my clarification and understanding :) 1, When disabling Nagle at sender, the sender TCP will not wait for the application to send enough data to fill the MSS. 2, But if the application send only 7 blocks of data to Sending TCP and due to Delayed ACK implementation at the receiver side, this seventh segment can only be acknowledged once the timer expires :( 3, At this same time the Sending application can only send data to TCP after receiving acknowledgment for the seventh segment :( so to speed up the process delayed ack must also be disabled at receiver.</p><p>Did I get your point?</p><p>Thanks</p></div><div id="comment-57914-info" class="comment-info"><span class="comment-age">(06 Dec '16, 13:16)</span> soochi</div></div><span id="57915"></span><div id="comment-57915" class="comment"><div id="post-57915-score" class="comment-score"></div><div class="comment-text"><p>The receiver is - luckily - not delaying acknowledgements.<br />
The delays we see here is the RTT so it could be even worse if delayACKs was enabled at the receiver. So disabling Nagle at the sender alone should take care of this ... (I guess) Please report back the results, once you've tired this</p></div><div id="comment-57915-info" class="comment-info"><span class="comment-age">(06 Dec '16, 14:50)</span> mrEEde</div></div><span id="58005"></span><div id="comment-58005" class="comment"><div id="post-58005-score" class="comment-score"></div><div class="comment-text"><p>The Nagle algorithm does not come into effect here.</p><p>Because the sender sends a non full MSS segment with PSH bit set even if the bytes in flight is non zero.</p><p>There is more gap between the segments with full MSS comparing to non full MSS</p></div><div id="comment-58005-info" class="comment-info"><span class="comment-age">(11 Dec '16, 08:35)</span> soochi</div></div></div><div id="comment-tools-57836" class="comment-tools"></div><div class="clear"></div><div id="comment-57836-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<div class="paginator-container-left">

</div>

</div>

</div>


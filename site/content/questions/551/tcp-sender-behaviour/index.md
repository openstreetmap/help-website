+++
type = "question"
title = "TCP sender behaviour"
description = '''/PUSH anybody an idea?  Hi everybody, during my recent analysis job, analyzing a TCP sender&#x27;s burst behaviour was a challenging task. Short form of my question: Does anybody know if TCPs congestion management algorithms concerning cwnd, ssthresh and the congestion avoidance / fast recovery technique...'''
date = "2010-10-20T03:04:00Z"
lastmod = "2010-11-15T03:09:00Z"
weight = 551
keywords = [ "drops", "cwnd", "tcp", "packet", "congestion" ]
aliases = [ "/questions/551" ]
osqa_answers = 1
osqa_accepted = true
+++

<div class="headNormal">

# [TCP sender behaviour](/questions/551/tcp-sender-behaviour)

</div>

<div id="main-body">

<div id="askform">

<table id="question-table" style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><span id="post-551-upvote" class="ajax-command post-vote up" rel="nofollow" title="I like this post (click again to cancel)"> </span><div id="post-551-score" class="post-score" title="current number of votes">1</div><span id="post-551-downvote" class="ajax-command post-vote down" rel="nofollow" title="I dont like this post (click again to cancel)"> </span> <span id="favorite-mark" class="ajax-command favorite-mark" rel="nofollow" title="mark/unmark this question as favorite (click again to cancel)"> </span><div id="favorite-count" class="favorite-count">1</div></div></td><td><div id="item-right"><div class="question-body"><p>/PUSH</p><p>anybody an idea?</p><hr /><p>Hi everybody,</p><p>during my recent analysis job, analyzing a TCP sender's burst behaviour was a challenging task.</p><p><em>Short form of my question:</em> Does anybody know if TCPs congestion management algorithms concerning cwnd, ssthresh and the congestion avoidance / fast recovery techniques apply to every single TCP connection regardless of its burstiness ?</p><p>Example leading to my question: Following the TCP related RFCs, congestion management / avoidance mechanisms in TCP and the extended definitions like e.g. NewReno - the sending device follows several algorithms to determine its "sending speed" respectively the number of packets it puts onto the wire before waiting for an ACK.</p><p>During recent analysis I came across a typical downlink from 1 Gig to 100M where the sending servers on the Gig Link startet by sending loads of packets (50+ packets) at full line rate which hurt buffers on the access switch.</p><p>If i get RFC specification of congestion avoidance right, every acknowledged packet (for slow start) increases cwnd and by that the number of bytes the sender can put on the wire - only limited by client window size.</p><p><em>Key question:</em> Is that algorithm also used when several small request/response packets are exchanged ? I can hardly imagine, that this is the way it should work - like for example 200 requests and 200 responses with a decent time between would push cwnd up to a very high value, resulting in a tsunami-like burst of packets when afterwards a big file for example would get requested...</p><p>There are lots of more detailed questions concerning the whole sender's behaviour, but that one is one of the most important to me.</p><p>Has anyone dealt with TCP sender's behaviour in that depth and could give me a hint ?</p><p>Regards, Landi</p></div><div id="question-tags" class="tags-container tags"><span class="post-tag tag-link-drops" rel="tag" title="see questions tagged &#39;drops&#39;">drops</span> <span class="post-tag tag-link-cwnd" rel="tag" title="see questions tagged &#39;cwnd&#39;">cwnd</span> <span class="post-tag tag-link-tcp" rel="tag" title="see questions tagged &#39;tcp&#39;">tcp</span> <span class="post-tag tag-link-packet" rel="tag" title="see questions tagged &#39;packet&#39;">packet</span> <span class="post-tag tag-link-congestion" rel="tag" title="see questions tagged &#39;congestion&#39;">congestion</span></div><div id="question-controls" class="post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>asked <strong>20 Oct '10, 03:04</strong></p><img src="https://secure.gravatar.com/avatar/36b41326bff63eb5ad73a0436914e05c?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="Landi&#39;s gravatar image" /><p><span>Landi</span><br />
<span class="score" title="2269 reputation points"><span>2.3k</span></span><span title="5 badges"><span class="badge1">●</span><span class="badgecount">5</span></span><span title="14 badges"><span class="silver">●</span><span class="badgecount">14</span></span><span title="42 badges"><span class="bronze">●</span><span class="badgecount">42</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="Landi has 28 accepted answers">28%</span></p></div><div class="post-update-info post-update-info-edited"><p><span> edited <strong>09 Nov '10, 02:46</strong> </span></p></div></div><div id="comments-container-551" class="comments-container"></div><div id="comment-tools-551" class="comment-tools"></div><div class="clear"></div><div id="comment-551-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

------------------------------------------------------------------------

<div class="tabBar">

<span id="sort-top"></span>

<div class="headQuestions">

One Answer:

</div>

</div>

<span id="879"></span>

<div id="answer-container-879" class="answer accepted-answer">

<table style="width:100%;"><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><tbody><tr class="odd"><td style="width: 30px; vertical-align: top"><div class="vote-buttons"><span id="post-879-upvote" class="ajax-command post-vote up" rel="nofollow" title="I like this post (click again to cancel)"> </span><div id="post-879-score" class="post-score" title="current number of votes">3</div><span id="post-879-downvote" class="ajax-command post-vote down" rel="nofollow" title="I dont like this post (click again to cancel)"> </span> <span class="accept-answer on" rel="nofollow" title="Landi has selected this answer as the correct answer"> </span></div></td><td><div class="item-right"><div class="answer-body"><p>Good question!</p><p>Not every TCP session will follow the RFC guidelines for TCP behaviour. Why? Because of the diversity of logic amongst the various applications, stack shims, etc. It's been my experience that MOST of the application development environments (use whichever acronym you're used to) take vastly different stances on just how much control they take over the TCP stack. Most apps (again, IME) don't get into stack level controls - they let the stack do what it does best. In these situations you'll see TCP slow start, active window size adverts, CWND, etc. Another thing to consider is the age of the stack. In the <em>OLD</em> days TCP SlowStart would only be used if the remote end was part of a different subnet, now it seems that SS is almost always used in the current stacks.</p><p>When it comes to buffer size (advert'd window size), CWND, and send buffer - the least of the three is what TCP uses. You will USUALLY only see the receive buffer (advert'd window size) in the capture. You can't see the send buffer in the capture, but sometimes you can derive info related to CWND. To answer part of your question regarding CWND - it will grow exponentially with each round trip BUT once it reaches the size of the SEND or RECEIVE buffer the stack <em>should</em> choose whichever is smallest.</p><p>Have I muddied the waters enough? To put it simply - the RFCs are nice and informative, but it's rare that they are strictly followed in every implementation. It's also quite possible for programmers (app, kernel, stack, etc) to control communications directly and avoid all built-in TCP stack controls. MS CIFS is a prime example of this - it follows no rules but it's own.</p></div><div class="answer-controls post-controls"></div><div class="post-update-info-container"><div class="post-update-info post-update-info-user"><p>answered <strong>09 Nov '10, 08:16</strong></p><img src="https://secure.gravatar.com/avatar/9e493496d59bb4ce33c37cd6e7a26a4d?s=32&amp;d=identicon&amp;r=g" class="gravatar" width="32" height="32" alt="GeonJay&#39;s gravatar image" /><p><span>GeonJay</span><br />
<span class="score" title="470 reputation points">470</span><span title="5 badges"><span class="badge1">●</span><span class="badgecount">5</span></span><span title="9 badges"><span class="silver">●</span><span class="badgecount">9</span></span><span title="22 badges"><span class="bronze">●</span><span class="badgecount">22</span></span><br />
<span class="accept_rate" title="Rate of the user&#39;s accepted answers">accept rate:</span> <span title="GeonJay has 2 accepted answers">5%</span></p></div></div><div id="comments-container-879" class="comments-container"><span id="881"></span><div id="comment-881" class="comment"><div id="post-881-score" class="comment-score"></div><div class="comment-text"><p>THANKS a lot for stepping into the deep water where my question is located at ;)</p><p>Your answers are 100% clear to me - what I was looking for is a step further into the inside of the TCP stack doing "flow control".</p><p>Let me split my question into 2 smaller but more specific ones:</p></div><div id="comment-881-info" class="comment-info"><span class="comment-age">(09 Nov '10, 08:35)</span> <span class="comment-user userinfo">Landi</span></div></div><span id="882"></span><div id="comment-882" class="comment"><div id="post-882-score" class="comment-score"></div><div class="comment-text"><ol><li>Do you know IF (!) slow start and later congestion avoidance after reaching sstresh also apply and by that enlarge CWND when there is a request-response oriented TCP session without loads of traffic ?</li></ol><p>Example: TCP Handshake and after that Client requesting something every 500ms, Server answering every Request with &lt; 1000 Byte. Would that grow CWND to the max. (which is recieve window of client) ?</p><p>That would mean, the next request for a HUGE file inside that sessions would result in the server sending a full r-wnd w/o waiting for a single ACK --&gt; Vista 2MB r-wnd --&gt; "Tsunami" :)</p></div><div id="comment-882-info" class="comment-info"><span class="comment-age">(09 Nov '10, 08:38)</span> <span class="comment-user userinfo">Landi</span></div></div><span id="883"></span><div id="comment-883" class="comment"><div id="post-883-score" class="comment-score"></div><div class="comment-text"><p>2nd: I heard the term "ACK-clocked" so often: When a server on a Gigabit link sees incoming ACKs every ~ 125msec (100MBit intervals) why the hell would he continue sending more and more and more data only because the recieve window of the client (e.g. 2MByte on Vista) allows it? I have the trace file where the bytes in flight are sky-rocketing and the server (SMB) is still pushing, because r-wnd. ALLOWS it in theory...</p><p>Is there a key point I am missing, or is there really nothing else then making the recieve window really small to stop those servers from overfilling switch buffers?!</p></div><div id="comment-883-info" class="comment-info"><span class="comment-age">(09 Nov '10, 08:41)</span> <span class="comment-user userinfo">Landi</span></div></div><span id="884"></span><div id="comment-884" class="comment"><div id="post-884-score" class="comment-score"></div><div class="comment-text"><p>Both of my answers are based on an RFC compliant stack. 1) Yes, SS is applied to each connection. Once you reach the SSThresh OR the Client's Advert'd RCV window size you will switch from SS to CA. If we start with an MSS of 1460 then each recv'd ACK will double the window (1460, 2920, 5840, etc) until we hit the RECV window. If Window Scaling is used (evidenced by Vista's 2MB r-wnd) then it's fully possible for the server to drop enough data on the wire to fill the advert'd RECV window - the 2MB Tsunami that you mentioned. Most workstations can't handle that kind of load being dropped.</p></div><div id="comment-884-info" class="comment-info"><span class="comment-age">(09 Nov '10, 10:07)</span> <span class="comment-user userinfo">GeonJay</span></div></div><span id="885"></span><div id="comment-885" class="comment"><div id="post-885-score" class="comment-score"></div><div class="comment-text"><p>2) My friend, this is the world we live in. Should we limit the sale of beer to an individual because they're alcoholic? The servers assume that client can handle the data because the window is so large. We have a large Solaris system here that will, quite often, drop ~50MB of data on the wire to a user laptop. In the trace you can see the large R-WND Advert coming from the client, you then see the HUGE data dump come down the pipe, then you see nothing but ZeroWindow adverts coming from the client for a few cycles. Your option is to artificially limit the Windows on either side.</p></div><div id="comment-885-info" class="comment-info"><span class="comment-age">(09 Nov '10, 10:12)</span> <span class="comment-user userinfo">GeonJay</span></div></div><span id="890"></span><div id="comment-890" class="comment not_top_scorer"><div id="post-890-score" class="comment-score"></div><div class="comment-text"><p>Another option is implement qos on the switch. The rate-limiting or policing can help by dropping enough packets to slow down TCP. Although QoS can be complicated (in Cisco world, you have line card, supervisor, and IOS dependencies to worry about), rate limiting or policing traffic so you don't overrun the receiver can be done. Especially if you're not talking about n-way combination. One question though, is the spiral of death (caused by a torrent of zero windows?) causing performance issues? If there's no contention on the wire, letting it run as fast as possible may not be an issue.</p></div><div id="comment-890-info" class="comment-info"><span class="comment-age">(09 Nov '10, 15:14)</span> <span class="comment-user userinfo">hansangb</span></div></div><span id="955"></span><div id="comment-955" class="comment not_top_scorer"><div id="post-955-score" class="comment-score"></div><div class="comment-text"><p>@ hansangb: QoS on the switch is absolutely no option in this current case. The interesting thing is, that the TCP Stack of the client Vista is not impressed at all by large packet drops and keeps advertising it's huge recieve window. This results in a very predictable packet drop load, once the server recovers from the fast recovery and/or slow start.</p><p>What I read about rate limiting officially from MS is that the way to handle it is limit sending rate by software QoS on the MS servers... pretty interested how and if this works...</p><p>Thus your point is interesting, thanks a lot for the input.</p></div><div id="comment-955-info" class="comment-info"><span class="comment-age">(15 Nov '10, 03:03)</span> <span class="comment-user userinfo">Landi</span></div></div><span id="956"></span><div id="comment-956" class="comment not_top_scorer"><div id="post-956-score" class="comment-score"></div><div class="comment-text"><p>@GeonJay Thanks for keeping up with my questions. I already feared that user r-wnd would be the only current issue - though (refer to my comment above) officially server rate limiting should do as well but is not so easy applicable.</p><p>Now I look forward to seeing huge loads of SMB(2) traffic bombs after SMBs initial "smalltalk" blows the cwnd for Vista/7 clients to &gt; 100k per session :-/</p><p>I might post another topic, if something else comes up - will analyse this issue the next month in detail.</p><p>THANKS a lot so far</p></div><div id="comment-956-info" class="comment-info"><span class="comment-age">(15 Nov '10, 03:09)</span> <span class="comment-user userinfo">Landi</span></div></div></div><div id="comment-tools-879" class="comment-tools"><span class="comments-showing"> showing 5 of 8 </span> <a href="#" class="show-all-comments-link">show 3 more comments</a></div><div class="clear"></div><div id="comment-879-form-container" class="comment-form-container"></div><div class="clear"></div></div></td></tr></tbody></table>

</div>

<div class="paginator-container-left">

</div>

</hr>

</div>

</div>

